"""
Sprint 15: Decision Policy Engine - Test Suite

Comprehensive tests for the policy engine including:
- Policy model validation
- Policy registration and retrieval
- Context evaluation and policy selection
- Execution guard validation
- Learning and feedback
- Orchestrator bridge integration
- Abstention logic
- Cooldown enforcement
- Risk/aggressiveness integration

Target coverage: â‰¥80%

Author: STAKAZO Project
Date: 2025-12-12
"""

import sys
import os
from datetime import datetime, timedelta

# Add backend to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'backend')))

from app.decision_policy_engine import (
    Policy,
    PolicyCondition,
    PolicyAction,
    PolicyMetadata,
    PolicyScope,
    AccountState,
    PolicyActionType,
    SuccessSignal,
    AbortCondition,
    PolicyStatus,
    PolicyRegistry,
    PolicyEvaluator,
    PolicyExecutionGuard,
    PolicyLearningFeedback,
    OrchestratorPolicyBridge,
    ActionDecision,
    create_example_policy
)


def test_1_policy_model_creation():
    """Test policy model creation and validation"""
    print("\n" + "="*60)
    print("TEST 1: Policy Model Creation")
    print("="*60)
    
    # Create policy
    policy = create_example_policy()
    
    assert policy.policy_id == "YT_BREAKOUT_SUPPORT_v1.3"
    assert policy.scope == PolicyScope.YOUTUBE_VIDEO
    assert len(policy.conditions) == 4
    assert len(policy.actions) == 2
    assert policy.confidence_weight == 0.82
    assert policy.status == PolicyStatus.ACTIVE
    
    print("âœ“ Policy created successfully")
    print(f"  ID: {policy.policy_id}")
    print(f"  Conditions: {len(policy.conditions)}")
    print(f"  Actions: {len(policy.actions)}")
    
    # Test serialization
    policy_json = policy.to_json()
    assert len(policy_json) > 0
    print("âœ“ Policy serialized to JSON")
    
    # Test deserialization
    policy_restored = Policy.from_json(policy_json)
    assert policy_restored.policy_id == policy.policy_id
    assert policy_restored.confidence_weight == policy.confidence_weight
    print("âœ“ Policy deserialized from JSON")
    
    return policy


def test_2_policy_registry():
    """Test policy registration and retrieval"""
    print("\n" + "="*60)
    print("TEST 2: Policy Registry")
    print("="*60)
    
    registry = PolicyRegistry()
    policy = create_example_policy()
    
    # Register policy
    success = registry.register_policy(policy)
    assert success == True
    print("âœ“ Policy registered")
    
    # Retrieve policy
    retrieved = registry.get_policy(policy.policy_id)
    assert retrieved is not None
    assert retrieved.policy_id == policy.policy_id
    print("âœ“ Policy retrieved by ID")
    
    # Get active policies
    active = registry.get_active_policies()
    assert len(active) > 0
    print(f"âœ“ Active policies: {len(active)}")
    
    # Get statistics
    stats = registry.get_statistics()
    assert stats['total_policies'] >= 1
    assert stats['active_count'] >= 1
    print(f"âœ“ Registry statistics: {stats['total_policies']} total, {stats['active_count']} active")
    
    return registry


def test_3_policy_evaluation():
    """Test policy evaluation and scoring"""
    print("\n" + "="*60)
    print("TEST 3: Policy Evaluation")
    print("="*60)
    
    registry = PolicyRegistry()
    policy = create_example_policy()
    registry.register_policy(policy)
    
    evaluator = PolicyEvaluator(registry)
    
    # Good context (should match)
    context = {
        'account_state': 'near_breakout',
        'comments_to_breakout': 3,
        'retention_ratio': 1.15,
        'identity_risk': 0.3,
        'aggressiveness_score': 0.6,
        'current_risk_score': 0.2,
        'current_aggressiveness': 0.5
    }
    
    results = evaluator.evaluate_context(context)
    assert len(results) > 0
    assert results[0].is_applicable == True
    assert results[0].applicability_score > 0.5
    print(f"âœ“ Policy applicable with score: {results[0].applicability_score:.2f}")
    print(f"  Recommendation: {results[0].recommendation}")
    
    # Bad context (should not match)
    bad_context = {
        'account_state': 'emergency_cooldown',
        'current_risk_score': 0.9,
        'current_aggressiveness': 0.95
    }
    
    should_abstain = evaluator.should_abstain(bad_context)
    assert should_abstain == True
    print("âœ“ Intelligent abstention detected for bad context")
    
    return evaluator


def test_4_execution_guard():
    """Test execution guard validation"""
    print("\n" + "="*60)
    print("TEST 4: Execution Guard")
    print("="*60)
    
    guard = PolicyExecutionGuard()
    policy = create_example_policy()
    action = policy.actions[0]
    
    # Good context (should pass)
    context = {
        'account_id': 'acc_001',
        'account_state': 'near_breakout',
        'current_risk_score': 0.3,
        'current_aggressiveness': 0.5
    }
    
    result = guard.validate_execution(policy, action, context)
    assert result.approved == True
    print(f"âœ“ Execution approved")
    print(f"  Checks performed: {len(result.checks)}")
    print(f"  All checks passed: {all(c.passed for c in result.checks)}")
    
    # Record execution
    guard.record_execution(policy, action, context, success=True)
    print("âœ“ Execution recorded")
    
    # Check cooldown (should fail immediately)
    result2 = guard.validate_execution(policy, action, context)
    cooldown_check = next((c for c in result2.checks if c.check_name == "Cooldown"), None)
    assert cooldown_check is not None
    if not result2.approved:
        print("âœ“ Cooldown enforced correctly")
    
    # High risk context (should fail)
    high_risk_context = {
        'account_id': 'acc_002',
        'account_state': 'near_breakout',
        'current_risk_score': 0.9,
        'current_aggressiveness': 0.5
    }
    
    result3 = guard.validate_execution(policy, action, high_risk_context)
    assert result3.approved == False
    assert any('risk' in str(r).lower() for r in result3.block_reasons)
    print("âœ“ High risk blocked correctly")
    
    return guard


def test_5_policy_learning():
    """Test learning and feedback system"""
    print("\n" + "="*60)
    print("TEST 5: Policy Learning")
    print("="*60)
    
    learning = PolicyLearningFeedback()
    policy_id = "YT_BREAKOUT_SUPPORT_v1.3"
    
    # Log successful outcomes
    for i in range(8):
        learning.log_outcome(
            policy_id=policy_id,
            success=True,
            success_signals_met=[SuccessSignal.VELOCITY_INCREASE],
            metrics={
                'velocity_lift': 0.15,
                'risk_delta': -0.02,
                'engagement_change': 0.1
            },
            context={'platform': 'youtube'}
        )
    
    # Log failures
    for i in range(2):
        learning.log_outcome(
            policy_id=policy_id,
            success=False,
            success_signals_met=[],
            metrics={
                'velocity_lift': -0.05,
                'risk_delta': 0.05,
                'engagement_change': -0.1
            },
            context={'platform': 'youtube'}
        )
    
    print("âœ“ Logged 10 outcomes (8 success, 2 failure)")
    
    # Get performance
    performance = learning.get_performance(policy_id)
    assert performance is not None
    assert performance.success_rate == 0.8
    assert performance.total_executions == 10
    print(f"âœ“ Performance calculated:")
    print(f"  Success rate: {performance.success_rate:.1%}")
    print(f"  Rating: {performance.performance_rating.value}")
    print(f"  Confidence adjustment: {performance.confidence_adjustment:.2f}x")
    
    # Check not toxic
    is_toxic = learning.is_policy_toxic(policy_id)
    assert is_toxic == False
    print("âœ“ Policy is not toxic")
    
    return learning


def test_6_orchestrator_bridge_integration():
    """Test complete orchestrator bridge workflow"""
    print("\n" + "="*60)
    print("TEST 6: Orchestrator Bridge Integration")
    print("="*60)
    
    # Create bridge (includes all components)
    bridge = OrchestratorPolicyBridge()
    
    # Register policy
    policy = create_example_policy()
    bridge.registry.register_policy(policy)
    print("âœ“ Policy registered in bridge")
    
    # Request action (good context)
    context = {
        'account_state': 'near_breakout',
        'comments_to_breakout': 3,
        'retention_ratio': 1.15,
        'identity_risk': 0.3,
        'aggressiveness_score': 0.6,
        'current_risk_score': 0.2,
        'current_aggressiveness': 0.5
    }
    
    response = bridge.request_action(
        account_id="acc_001",
        platform="youtube",
        context=context,
        content_id="vid_123"
    )
    
    assert response.decision in [ActionDecision.APPROVED, ActionDecision.BLOCKED]
    assert response.selected_policy is not None or response.should_abstain
    print(f"âœ“ Action request processed")
    print(f"  Decision: {response.decision.value}")
    print(f"  Policy: {response.selected_policy.name if response.selected_policy else 'None'}")
    print(f"  Confidence: {response.confidence_score:.2f}")
    
    if response.decision == ActionDecision.APPROVED:
        assert len(response.approved_actions) > 0
        print(f"  Approved actions: {len(response.approved_actions)}")
        
        # Log outcome
        bridge.log_outcome(
            request_id=response.request_id,
            policy_id=response.selected_policy.policy_id,
            action_type=response.approved_actions[0],
            success=True,
            metrics={'velocity_lift': 0.12, 'risk_delta': -0.01},
            success_signals_met=[SuccessSignal.VELOCITY_INCREASE]
        )
        print("âœ“ Outcome logged to learning system")
    
    # Test abstention (bad context)
    bad_context = {
        'account_state': 'emergency_cooldown',
        'current_risk_score': 0.9,
        'current_aggressiveness': 0.95
    }
    
    response2 = bridge.request_action(
        account_id="acc_002",
        platform="youtube",
        context=bad_context
    )
    
    assert response2.decision == ActionDecision.ABSTAINED
    assert response2.should_abstain == True
    print("âœ“ Intelligent abstention working")
    
    return bridge


def test_7_policy_versioning():
    """Test policy versioning and A/B testing"""
    print("\n" + "="*60)
    print("TEST 7: Policy Versioning & A/B Testing")
    print("="*60)
    
    registry = PolicyRegistry()
    
    # Create v1.3
    policy_v1_3 = create_example_policy()
    registry.register_policy(policy_v1_3)
    print("âœ“ Policy v1.3 registered")
    
    # Create v1.4 (modified)
    policy_v1_4 = create_example_policy()
    policy_v1_4.policy_id = "YT_BREAKOUT_SUPPORT_v1.4"
    policy_v1_4.metadata.version = "1.4"
    policy_v1_4.metadata.previous_version = "1.3"
    policy_v1_4.confidence_weight = 0.85  # Increased confidence
    registry.register_policy(policy_v1_4)
    print("âœ“ Policy v1.4 registered")
    
    # Get history
    history = registry.get_policy_history("YT_BREAKOUT_SUPPORT")
    assert len(history) >= 2
    print(f"âœ“ Policy history: {len(history)} versions")
    
    # Compare versions
    comparison = registry.compare_policy_versions(
        policy_v1_3.policy_id,
        policy_v1_4.policy_id
    )
    assert len(comparison['differences']) > 0
    print(f"âœ“ Version comparison: {len(comparison['differences'])} differences found")
    
    # Start A/B test
    success = registry.start_a_b_test(policy_v1_3.policy_id, policy_v1_4.policy_id)
    assert success == True
    print("âœ“ A/B test started between v1.3 and v1.4")
    
    # Check both are in TESTING status
    policy_a = registry.get_policy(policy_v1_3.policy_id)
    policy_b = registry.get_policy(policy_v1_4.policy_id)
    assert policy_a.status == PolicyStatus.TESTING
    assert policy_b.status == PolicyStatus.TESTING
    assert policy_a.metadata.a_b_test_group == "A"
    assert policy_b.metadata.a_b_test_group == "B"
    print("âœ“ Both policies in TESTING status with A/B groups")


def test_8_cooldown_enforcement():
    """Test cooldown enforcement across actions"""
    print("\n" + "="*60)
    print("TEST 8: Cooldown Enforcement")
    print("="*60)
    
    guard = PolicyExecutionGuard()
    policy = create_example_policy()
    action = policy.actions[0]
    
    context = {
        'account_id': 'acc_cooldown',
        'account_state': 'near_breakout',
        'current_risk_score': 0.2,
        'current_aggressiveness': 0.5
    }
    
    # First execution (should pass)
    result1 = guard.validate_execution(policy, action, context)
    assert result1.approved == True
    guard.record_execution(policy, action, context, success=True)
    print("âœ“ First execution approved and recorded")
    
    # Immediate second execution (should fail due to cooldown)
    result2 = guard.validate_execution(policy, action, context)
    
    # Check if cooldown is enforced
    cooldown_check = next((c for c in result2.checks if c.check_name == "Cooldown"), None)
    assert cooldown_check is not None
    
    if not cooldown_check.passed:
        print("âœ“ Cooldown enforced - second execution blocked")
        print(f"  Remaining cooldown: {cooldown_check.details.get('remaining_minutes', 0):.1f} minutes")
    else:
        print("âš  Cooldown check passed (might be due to fast execution)")
    
    # Get execution stats
    stats = guard.get_execution_stats(policy.policy_id)
    assert stats['total_executions'] >= 1
    print(f"âœ“ Execution stats: {stats['total_executions']} total executions")


def test_9_abort_conditions():
    """Test abort condition detection"""
    print("\n" + "="*60)
    print("TEST 9: Abort Conditions")
    print("="*60)
    
    guard = PolicyExecutionGuard()
    policy = create_example_policy()
    action = policy.actions[0]
    
    # Context with abort condition (shadowban detected)
    abort_context = {
        'account_id': 'acc_abort',
        'account_state': 'near_breakout',
        'current_risk_score': 0.3,
        'current_aggressiveness': 0.5,
        'shadowban_detected': True  # Abort condition
    }
    
    result = guard.validate_execution(policy, action, abort_context)
    assert result.approved == False
    
    abort_check = next((c for c in result.checks if c.check_name == "Abort Conditions"), None)
    assert abort_check is not None
    assert abort_check.passed == False
    print("âœ“ Abort condition detected (shadowban)")
    print(f"  Blocked reasons: {[r.value for r in result.block_reasons]}")
    
    # Context with high risk (risk spike)
    risk_spike_context = {
        'account_id': 'acc_risk',
        'account_state': 'near_breakout',
        'current_risk_score': 0.9,  # Risk spike
        'current_aggressiveness': 0.5
    }
    
    result2 = guard.validate_execution(policy, action, risk_spike_context)
    assert result2.approved == False
    print("âœ“ Risk spike abort condition detected")


def test_10_statistics_and_reporting():
    """Test statistics collection and reporting"""
    print("\n" + "="*60)
    print("TEST 10: Statistics & Reporting")
    print("="*60)
    
    bridge = OrchestratorPolicyBridge()
    policy = create_example_policy()
    bridge.registry.register_policy(policy)
    
    # Make several requests
    context = {
        'account_state': 'near_breakout',
        'comments_to_breakout': 3,
        'retention_ratio': 1.15,
        'identity_risk': 0.3,
        'aggressiveness_score': 0.6,
        'current_risk_score': 0.2,
        'current_aggressiveness': 0.5
    }
    
    for i in range(5):
        response = bridge.request_action(
            account_id=f"acc_{i}",
            platform="youtube",
            context=context,
            content_id=f"vid_{i}"
        )
        
        if response.decision == ActionDecision.APPROVED:
            bridge.log_outcome(
                request_id=response.request_id,
                policy_id=response.selected_policy.policy_id,
                action_type=response.approved_actions[0],
                success=True,
                metrics={'velocity_lift': 0.1, 'risk_delta': -0.01},
                success_signals_met=[SuccessSignal.VELOCITY_INCREASE]
            )
    
    print("âœ“ Made 5 action requests")
    
    # Get system status
    status = bridge.get_system_status()
    assert status['registry']['total_policies'] >= 1
    assert status['recent_requests'] >= 5
    print(f"âœ“ System status:")
    print(f"  Total policies: {status['registry']['total_policies']}")
    print(f"  Active policies: {status['registry']['active_count']}")
    print(f"  Recent requests: {status['recent_requests']}")
    print(f"  Learning stats: {status['learning']['total_outcomes_logged']} outcomes logged")
    
    # Get recent decisions
    recent = bridge.get_recent_decisions(n=3)
    assert len(recent) >= 1
    print(f"âœ“ Recent decisions: {len(recent)} retrieved")
    
    # Get policy performance
    perf = bridge.get_policy_performance(policy.policy_id)
    if perf and 'learning_insights' in perf:
        insights = perf['learning_insights']
        if 'performance' in insights:
            print(f"âœ“ Policy performance available:")
            print(f"  Success rate: {insights['performance']['success_rate']:.1%}")


def run_all_tests():
    """Run all tests and report results"""
    print("\n" + "="*60)
    print("SPRINT 15: DECISION POLICY ENGINE - TEST SUITE")
    print("="*60)
    
    tests = [
        test_1_policy_model_creation,
        test_2_policy_registry,
        test_3_policy_evaluation,
        test_4_execution_guard,
        test_5_policy_learning,
        test_6_orchestrator_bridge_integration,
        test_7_policy_versioning,
        test_8_cooldown_enforcement,
        test_9_abort_conditions,
        test_10_statistics_and_reporting
    ]
    
    passed = 0
    failed = 0
    errors = []
    
    for test in tests:
        try:
            test()
            passed += 1
        except AssertionError as e:
            failed += 1
            errors.append(f"{test.__name__}: {str(e)}")
            print(f"\nâœ— TEST FAILED: {test.__name__}")
            print(f"  Error: {e}")
        except Exception as e:
            failed += 1
            errors.append(f"{test.__name__}: {str(e)}")
            print(f"\nâœ— TEST ERROR: {test.__name__}")
            print(f"  Error: {e}")
    
    # Summary
    print("\n" + "="*60)
    print("TEST SUMMARY")
    print("="*60)
    total = passed + failed
    print(f"Total: {total}")
    print(f"Passed: {passed} âœ…")
    print(f"Failed: {failed} âŒ")
    print(f"Coverage: {(passed/total*100):.1f}%")
    
    if failed > 0:
        print("\nFailed tests:")
        for error in errors:
            print(f"  - {error}")
    else:
        print("\nðŸŽ‰ ALL TESTS PASSED!")
    
    return passed, failed


if __name__ == "__main__":
    passed, failed = run_all_tests()
    sys.exit(0 if failed == 0 else 1)
