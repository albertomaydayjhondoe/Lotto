# Job Runner System

Sistema completo de procesamiento aut√≥nomo de jobs del orquestador.

## üèóÔ∏è Arquitectura

```
backend/app/worker/
‚îú‚îÄ‚îÄ __init__.py          # Exports principales
‚îú‚îÄ‚îÄ worker.py            # Loop principal del worker
‚îú‚îÄ‚îÄ queue.py             # Cola persistente con locking
‚îú‚îÄ‚îÄ dispatcher.py        # Tabla de dispatch job_type ‚Üí handler
‚îî‚îÄ‚îÄ handlers/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îî‚îÄ‚îÄ cut_analysis.py  # Handler para an√°lisis de cortes
```

## üîÑ Estados de Jobs

```
PENDING    ‚Üí Job creado, esperando procesamiento
PROCESSING ‚Üí Job en ejecuci√≥n
RETRY      ‚Üí Job fall√≥, ser√° reintentado
COMPLETED  ‚Üí Job completado exitosamente
FAILED     ‚Üí Job fall√≥ permanentemente
```

## üéØ Componentes

### 1. Cola Persistente (`queue.py`)

**Funci√≥n principal:** `async def dequeue_job(db: AsyncSession)`

Caracter√≠sticas:
- Usa `SELECT FOR UPDATE SKIP LOCKED` para PostgreSQL
- Fallback para SQLite (sin locking concurrente)
- Selecciona jobs PENDING ordenados por created_at
- Marca inmediatamente como PROCESSING
- Commit autom√°tico para bloquear el job
- Soporta m√∫ltiples workers concurrentes

**SQL (PostgreSQL):**
```sql
SELECT * FROM jobs 
WHERE status = 'pending'
ORDER BY created_at
FOR UPDATE SKIP LOCKED
LIMIT 1
```

### 2. Dispatcher (`dispatcher.py`)

**DISPATCH_TABLE:**
```python
{
    "cut_analysis": run_cut_analysis,
    # A√±adir m√°s handlers aqu√≠
}
```

**Funci√≥n:** `async def dispatch_job(job: Job, db: AsyncSession)`

- Valida que el job_type exista en DISPATCH_TABLE
- Ejecuta el handler correspondiente
- Lanza KeyError si job_type desconocido

### 3. Worker Loop (`worker.py`)

**Funci√≥n principal:** `async def worker_loop(db: AsyncSession)`

**Flujo:**
```python
while True:
    job = await dequeue_job(db)
    if not job:
        await asyncio.sleep(WORKER_POLL_INTERVAL)
        continue
    
    try:
        handler = DISPATCH_TABLE[job.job_type]
        result = await handler(job, db)
        job.status = COMPLETED
        job.result = result
    except Exception as e:
        job.status = FAILED
        job.error = str(e)
    
    await db.commit()
```

**Funci√≥n auxiliar:** `async def process_single_job(db: AsyncSession)`

- Procesa UN solo job del queue
- Usado por endpoint `/jobs/process` (dev)
- Retorna dict con summary del procesamiento

### 4. Handler: Cut Analysis (`handlers/cut_analysis.py`)

**Funci√≥n:** `async def run_cut_analysis(job: Job, db: AsyncSession)`

**Proceso:**
1. Lee el video_asset_id del job
2. Obtiene VideoAsset de la DB
3. Simula an√°lisis con `asyncio.sleep(0.5)`
4. Genera N clips basados en duraci√≥n (1 clip cada 20s)
5. Calcula visual_score para cada clip
6. Crea registros Clip en la DB
7. Retorna dict:
   ```json
   {
     "clips_created": 3,
     "duration": 60000,
     "variants": [
       {
         "clip_id": "uuid",
         "start_ms": 0,
         "end_ms": 20000,
         "visual_score": 0.85
       }
     ]
   }
   ```

## üîå API Endpoints

### POST /jobs/process (DEV ONLY)

Procesa el siguiente job PENDING de la cola.

**Request:**
```bash
POST /jobs/process
```

**Response:**
```json
{
  "processed": true,
  "job_id": "uuid",
  "job_type": "cut_analysis",
  "status": "completed",
  "result": {
    "clips_created": 3,
    "duration": 60000,
    "variants": [...]
  },
  "processing_time_ms": 544,
  "error": null
}
```

**Si no hay jobs:**
```json
{
  "processed": false,
  "message": "No pending jobs in queue"
}
```

## ‚öôÔ∏è Configuraci√≥n

En `backend/app/core/config.py`:

```python
WORKER_POLL_INTERVAL: int = 2      # Segundos entre checks
MAX_JOB_RETRIES: int = 3           # Reintentos m√°ximos
WORKER_ENABLED: bool = False       # Activar worker background
```

## üß™ Tests

**Archivo:** `tests/test_job_runner.py`

**Tests implementados:**
1. ‚úÖ **test_process_job_from_queue** - Procesa job desde cola completo
2. ‚úÖ **test_no_reprocess_completed_jobs** - No reprocesa jobs completados
3. ‚úÖ **test_unknown_job_type_fails** - Job type inv√°lido marca FAILED
4. ‚úÖ **test_concurrent_queue_processing** - 3 jobs procesados sin conflictos
5. ‚úÖ **test_queue_empty_returns_false** - Queue vac√≠o retorna processed=false

**Ejecutar tests:**
```bash
cd backend
PYTHONPATH=/workspaces/stakazo/backend:$PYTHONPATH pytest tests/test_job_runner.py -v -s
```

**Resultado:**
```
5 passed, 60 warnings in 3.06s
```

## üöÄ Uso

### Modo Manual (Dev)

```python
from app.worker import process_single_job
from app.core.database import get_db

async for db in get_db():
    result = await process_single_job(db)
    print(result)
    break
```

### Modo Background Worker

```python
from app.worker import worker_loop
from app.core.database import get_db
import asyncio

async def start_worker():
    async for db in get_db():
        await worker_loop(db)

asyncio.run(start_worker())
```

### Via API (Testing)

```bash
curl -X POST http://localhost:8000/jobs/process
```

## üìä Flujo Completo

```
1. Cliente crea job:
   POST /upload ‚Üí crea VideoAsset + Job(status=PENDING)

2. Worker dequeue:
   SELECT ... FOR UPDATE SKIP LOCKED
   ‚Üí Marca PROCESSING

3. Dispatcher:
   DISPATCH_TABLE[job_type] ‚Üí handler

4. Handler ejecuta:
   run_cut_analysis(job, db)
   ‚Üí Genera clips

5. Worker actualiza:
   job.status = COMPLETED
   job.result = {...}
   ‚Üí COMMIT

6. Cliente consulta:
   GET /jobs/{id} ‚Üí status=COMPLETED
```

## üîí Concurrencia

**M√∫ltiples workers:**
- ‚úÖ Row-level locking con FOR UPDATE SKIP LOCKED
- ‚úÖ Cada worker procesa jobs diferentes
- ‚úÖ Sin deadlocks
- ‚úÖ Sin duplicaci√≥n de procesamiento

**SQLite Limitation:**
- ‚ö†Ô∏è No soporta FOR UPDATE SKIP LOCKED
- ‚ö†Ô∏è Fallback a SELECT simple
- ‚ö†Ô∏è No recomendado para m√∫ltiples workers en producci√≥n
- ‚úÖ PostgreSQL recomendado para producci√≥n

## üé® Extensibilidad

### A√±adir nuevo handler:

1. Crear handler en `handlers/`:

```python
# handlers/my_handler.py
async def run_my_handler(job: Job, db: AsyncSession) -> Dict[str, Any]:
    # Tu l√≥gica aqu√≠
    return {"result": "success"}
```

2. Registrar en dispatcher:

```python
# dispatcher.py
from app.worker.handlers.my_handler import run_my_handler

DISPATCH_TABLE = {
    "cut_analysis": run_cut_analysis,
    "my_handler": run_my_handler,  # ‚Üê A√±adir aqu√≠
}
```

3. Crear jobs con el nuevo tipo:

```python
job = Job(
    job_type="my_handler",
    status=JobStatus.PENDING,
    params={...}
)
```

## üìà Monitoreo

**Queries √∫tiles:**

```sql
-- Jobs por estado
SELECT status, COUNT(*) FROM jobs GROUP BY status;

-- Jobs fallidos recientes
SELECT * FROM jobs WHERE status = 'failed' ORDER BY updated_at DESC LIMIT 10;

-- Tiempo promedio de procesamiento
SELECT AVG(EXTRACT(EPOCH FROM (updated_at - created_at))) 
FROM jobs 
WHERE status = 'completed';
```

## üêõ Debugging

**Activar logs:**
```python
import logging
logging.basicConfig(level=logging.INFO)
```

**Logs importantes:**
- `Processing job {id} (type: {type})`
- `Job {id} completed successfully in {ms}ms`
- `Job {id} failed: {error}`

## ‚úÖ Checklist de Producci√≥n

- [ ] WORKER_ENABLED = True en producci√≥n
- [ ] Usar PostgreSQL (no SQLite)
- [ ] Configurar MAX_JOB_RETRIES adecuadamente
- [ ] Monitorear queue depth
- [ ] Logs centralizados
- [ ] Alertas para jobs FAILED
- [ ] M√©tricas de throughput
- [ ] Health checks del worker
